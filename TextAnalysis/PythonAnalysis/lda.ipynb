{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "# Read data into papers\n",
                "papers = pd.read_csv('../Experiments/metadata/songsMetadata.csv')\n",
                "# Print head\n",
                "papers.head()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>artistId</th>\n",
                            "      <th>geniusName</th>\n",
                            "      <th>songShortcut</th>\n",
                            "      <th>title</th>\n",
                            "      <th>songId</th>\n",
                            "      <th>url</th>\n",
                            "      <th>lyrics</th>\n",
                            "      <th>releaseDate</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>11774</td>\n",
                            "      <td>20Syl</td>\n",
                            "      <td>20SYL1</td>\n",
                            "      <td>100% Autoproduction by Hocus Pocus</td>\n",
                            "      <td>160272</td>\n",
                            "      <td>https://genius.com/Hocus-pocus-100-autoproduct...</td>\n",
                            "      <td>\\n          \\n            \\n            [Scrat...</td>\n",
                            "      <td>000000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>11774</td>\n",
                            "      <td>20Syl</td>\n",
                            "      <td>20SYL2</td>\n",
                            "      <td>100 Grammes de Peur by Hocus Pocus</td>\n",
                            "      <td>108615</td>\n",
                            "      <td>https://genius.com/Hocus-pocus-100-grammes-de-...</td>\n",
                            "      <td>\\n          \\n            \\n            On la ...</td>\n",
                            "      <td>000000000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>11774</td>\n",
                            "      <td>20Syl</td>\n",
                            "      <td>20SYL3</td>\n",
                            "      <td>10 que tu penses by Hocus Pocus</td>\n",
                            "      <td>2319837</td>\n",
                            "      <td>https://genius.com/Hocus-pocus-10-que-tu-pense...</td>\n",
                            "      <td>\\n          \\n            \\n            [Intro...</td>\n",
                            "      <td>January 1, 1998</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>11774</td>\n",
                            "      <td>20Syl</td>\n",
                            "      <td>20SYL4</td>\n",
                            "      <td>10 YRS by End of the Weak France (Ft. Artik (Q...</td>\n",
                            "      <td>438039</td>\n",
                            "      <td>https://genius.com/End-of-the-weak-france-10-y...</td>\n",
                            "      <td>\\n          \\n            \\n            [Artik...</td>\n",
                            "      <td>February 26, 2014</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>11774</td>\n",
                            "      <td>20Syl</td>\n",
                            "      <td>20SYL6</td>\n",
                            "      <td>73 Touches by Hocus Pocus</td>\n",
                            "      <td>61676</td>\n",
                            "      <td>https://genius.com/Hocus-pocus-73-touches-lyrics</td>\n",
                            "      <td>\\n          \\n            \\n            [Coupl...</td>\n",
                            "      <td>000000000</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   artistId geniusName songShortcut  \\\n",
                            "0     11774      20Syl       20SYL1   \n",
                            "1     11774      20Syl       20SYL2   \n",
                            "2     11774      20Syl       20SYL3   \n",
                            "3     11774      20Syl       20SYL4   \n",
                            "4     11774      20Syl       20SYL6   \n",
                            "\n",
                            "                                               title   songId  \\\n",
                            "0                 100% Autoproduction by Hocus Pocus   160272   \n",
                            "1                 100 Grammes de Peur by Hocus Pocus   108615   \n",
                            "2                    10 que tu penses by Hocus Pocus  2319837   \n",
                            "3  10 YRS by End of the Weak France (Ft. Artik (Q...   438039   \n",
                            "4                          73 Touches by Hocus Pocus    61676   \n",
                            "\n",
                            "                                                 url  \\\n",
                            "0  https://genius.com/Hocus-pocus-100-autoproduct...   \n",
                            "1  https://genius.com/Hocus-pocus-100-grammes-de-...   \n",
                            "2  https://genius.com/Hocus-pocus-10-que-tu-pense...   \n",
                            "3  https://genius.com/End-of-the-weak-france-10-y...   \n",
                            "4   https://genius.com/Hocus-pocus-73-touches-lyrics   \n",
                            "\n",
                            "                                              lyrics        releaseDate  \n",
                            "0  \\n          \\n            \\n            [Scrat...          000000000  \n",
                            "1  \\n          \\n            \\n            On la ...          000000000  \n",
                            "2  \\n          \\n            \\n            [Intro...    January 1, 1998  \n",
                            "3  \\n          \\n            \\n            [Artik...  February 26, 2014  \n",
                            "4  \\n          \\n            \\n            [Coupl...          000000000  "
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "# Remove the columns\n",
                "papers = papers.drop(columns=['artistId', 'geniusName', 'songShortcut', \n",
                "                              'title', 'songId', 'url', 'releaseDate'], axis=1)\n",
                "                              \n",
                "# Print out the first rows of papers\n",
                "papers.head()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>lyrics</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>\\n          \\n            \\n            [Scrat...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>\\n          \\n            \\n            On la ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>\\n          \\n            \\n            [Intro...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>\\n          \\n            \\n            [Artik...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>\\n          \\n            \\n            [Coupl...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                              lyrics\n",
                            "0  \\n          \\n            \\n            [Scrat...\n",
                            "1  \\n          \\n            \\n            On la ...\n",
                            "2  \\n          \\n            \\n            [Intro...\n",
                            "3  \\n          \\n            \\n            [Artik...\n",
                            "4  \\n          \\n            \\n            [Coupl..."
                        ]
                    },
                    "metadata": {},
                    "execution_count": 11
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "# Load the regular expression library\n",
                "import re\n",
                "# Remove punctuation\n",
                "papers['paper_text_processed'] = papers['lyrics'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
                "# Convert the titles to lowercase\n",
                "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
                "# Print out the first rows of papers\n",
                "papers['paper_text_processed'].head()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0    \\n          \\n            \\n            [scrat...\n",
                            "1    \\n          \\n            \\n            on la ...\n",
                            "2    \\n          \\n            \\n            [intro...\n",
                            "3    \\n          \\n            \\n            [artik...\n",
                            "4    \\n          \\n            \\n            [coupl...\n",
                            "Name: paper_text_processed, dtype: object"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 12
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "import gensim\n",
                "from gensim.utils import simple_preprocess\n",
                "def sent_to_words(sentences):\n",
                "    for sentence in sentences:\n",
                "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
                "data = papers.paper_text_processed.values.tolist()\n",
                "data_words = list(sent_to_words(data))\n",
                "print(data_words[:1][0][:30])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['scratch', 'dj', 'greem', 'ca', 'va', 'faire', 'un', 'sacre', 'boucan', 'refrain', 'cambia', 'syl', 'autoproduction', 'pour', 'un', 'son', 'pur', 'sang', 'empire', 'sans', 'empereur', 'ca', 'va', 'faire', 'un', 'sacre', 'boucan', 'couplet', 'cambia', 'cigare']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# Build the bigram and trigram models\n",
                "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
                "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
                "# Faster way to get a sentence clubbed as a trigram/bigram\n",
                "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
                "trigram_mod = gensim.models.phrases.Phraser(trigram)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# NLTK Stop words\n",
                "# import nltk\n",
                "# nltk.download('stopwords')\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "stop_words = stopwords.words('english') + stopwords.words('french')\n",
                "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
                "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
                "def remove_stopwords(texts):\n",
                "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
                "def make_bigrams(texts):\n",
                "    return [bigram_mod[doc] for doc in texts]\n",
                "def make_trigrams(texts):\n",
                "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
                "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
                "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
                "    texts_out = []\n",
                "    for sent in texts:\n",
                "        doc = nlp(\" \".join(sent)) \n",
                "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
                "    return texts_out"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "import spacy\n",
                "# Remove Stop Words\n",
                "data_words_nostops = remove_stopwords(data_words)\n",
                "# Form Bigrams\n",
                "data_words_bigrams = make_bigrams(data_words_nostops)\n",
                "# Initialize spacy 'fr' model, keeping only tagger component (for efficiency)\n",
                "nlp = spacy.load('fr_core_news_lg', disable=['parser', 'ner'])\n",
                "# Do lemmatization keeping only noun, adj, vb, adv\n",
                "data_lemmatized = lemmatization(data_words_bigrams)\n",
                "print(data_lemmatized[:1])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[['scratch_dj', 'greem', 'aller', 'sacre_boucan', 'refrain', 'cambia', 'autoproduction', 'pur', 'sang', 'empire', 'empereur', 'aller', 'sacre_boucan', 'couplet', 'cambia', 'cigare', 'bord', 'levre', 'verre', 'whisky', 'bureau', 'secretair', 'dessou', 'repense', 'dernier', 'blaireau', 'entube', 'dire', 'tenir', 'tube', 'recette', 'campagne', 'pub', 'sacem', 'reste', 'aime', 'producteur', 'rap', 'aller', 'croire', 'ca', 'gene', 'contraire', 'contrat', 'enfiler', 'perle', 'ecoute', 'nouveau', 'merde', 'groupe', 'rever', 'percer', 'bon', 'petit', 'truc', 'toucher', 'mal', 'montrer', 'fesse', 'faire', 'jeter', 'tact', 'producteur', 'charmer', 'vente', 'veux', 'seducteur', 'tenir', 'mec', 'venir', 'entrer', 'cassette', 'main', 'pouvoir', 'tube', 'prochain', 'veu', 'gamin', 'syl', 'vouloir', 'piscine', 'studio', 'sinon', 'cher', 'champagne', 'maillot', 'bain', 'plaisant', 'champagne', 'faire', 'voix', 'girl', 'enfin', 'voir', 'presente', 'tas', 'star', 'vouloir', 'phat', 'genre', 'limousine', 'boulevard', 'nom', 'son', 'radio', 'fans', 'face', 'maillot', 'arracher', 'crier', 'casser', 'cordes_vocale', 'arrache', 'scene', 'aprer', 'rappel', 'laisse', 'cassette', 'numero', 'vouloir', 'rappelles', 'reve', 'gonzer', 'pelle', 'refrain', 'cambia', 'autoproduction', 'pur', 'sang', 'empire', 'empereur', 'aller', 'sacre_boucan', 'couplet', 'cambia', 'ecoute', 'tout', 'demo', 'siffler', 'verre', 'whisky', 'casser', 'comprend', 'dire', 'parle', 'trop', 'vite', 'pouvoir', 'lacher', 'aller', 'faire', 'cassette', 'vierge', 'plus', 'chaine', 'manque', 'peu', 'funky', 'clap', 'claquement', 'doigt', 'bas', 'bouger', 'fille', 'boi', 'lette', 'chanter', 'mal', 'faire', 'choeur', 'plus', 'gonze', 'parle', 'dessu', 'appeler', 'rappeur', 'jeune', 'bon', 'filon', 'matier', 'malleabl', 'assez', 'agreabl', 'voir', 'esprits', 'aussi', 'influencable', 'enerve', 'trouve', 'antidote', 'vouloir', 'classe', 'top', 'eviter', 'flop', 'syl', 'opter', 'autoproduction', 'sortir', 'boite', 'hit', 'overdose', 'morceal', 'vite', 'faire', 'vite', 'vendre', 'mec', 'descendre', 'vendre', 'plus', 'mec', 'vivant', 'dur', 'sortir', 'truc', 'autoproduction', 'dent', 'der', 'conscient', 'major', 'mettre', 'plein', 'poche', 'rouler', 'jouer', 'alchimiste', 'garder', 'plomb', 'frais', 'temps', 'encore', 'represente', 'hip_hop', 'ouest', 'plein', 'essor', 'refrain', 'cambia', 'autoproduction', 'pur', 'sang', 'empire', 'empereur', 'aller', 'sacre_boucan']]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 134,
            "source": [
                "from gensim.models import CoherenceModel\n",
                "\n",
                "# supporting function\n",
                "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
                "    \n",
                "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
                "                                           id2word=dictionary,\n",
                "                                           num_topics=k, \n",
                "                                           random_state=100,\n",
                "                                           chunksize=100,\n",
                "                                           passes=10,\n",
                "                                           alpha=a,\n",
                "                                           eta=b)\n",
                "    \n",
                "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
                "    perplexity = lda_model.log_perplexity(corpus)\n",
                "    return [coherence_model_lda.get_coherence(), perplexity]\n",
                "\n",
                "\n",
                "import gensim.corpora as corpora\n",
                "# Create Dictionary\n",
                "id2word = corpora.Dictionary(data_lemmatized)\n",
                "# Create Corpus\n",
                "texts = data_lemmatized\n",
                "# Term Document Frequency\n",
                "corpus = [id2word.doc2bow(text) for text in texts]\n",
                "# View\n",
                "print(corpus[:1])\n",
                "num_of_docs = len(corpus)\n",
                "    \n",
                "import numpy as np\n",
                "import tqdm\n",
                "grid = {}\n",
                "grid['Validation_Set'] = {}\n",
                "# Topics range\n",
                "min_topics = 2\n",
                "max_topics = 30\n",
                "step_size = 1\n",
                "topics_range = range(min_topics, max_topics, step_size)\n",
                "# Alpha parameter\n",
                "alpha = list(np.arange(0.01, 1, 0.3))\n",
                "alpha.append('symmetric')\n",
                "alpha.append('asymmetric')\n",
                "# Beta parameter\n",
                "beta = list(np.arange(0.01, 1, 0.3))\n",
                "beta.append('symmetric')\n",
                "# Validation sets\n",
                "num_of_docs = len(corpus)\n",
                "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
                "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
                "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
                "               corpus]\n",
                "corpus_title = ['75% Corpus', '100% Corpus']\n",
                "model_results = {'Validation_Set': [],\n",
                "                 'Topics': [],\n",
                "                 'Alpha': [],\n",
                "                 'Beta': [],\n",
                "                 'Coherence': [],\n",
                "                 'Perplexity': []\n",
                "                }\n",
                "# Can take a long time to run\n",
                "if 1 == 1:\n",
                "    pbar = tqdm.tqdm(total=540)\n",
                "    \n",
                "    # iterate through validation corpuses\n",
                "    for i in range(len(corpus_sets)):\n",
                "        # iterate through number of topics\n",
                "        for k in topics_range:\n",
                "            # iterate through alpha values\n",
                "            for a in alpha:\n",
                "                # iterare through beta values\n",
                "                for b in beta:\n",
                "                    # get the coherence score for the given parameters\n",
                "                    results = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
                "                                                  k=k, a=a, b=b)\n",
                "                    # Save the model results\n",
                "                    model_results['Validation_Set'].append(corpus_title[i])\n",
                "                    model_results['Topics'].append(k)\n",
                "                    model_results['Alpha'].append(a)\n",
                "                    model_results['Beta'].append(b)\n",
                "                    model_results['Coherence'].append(results[0])\n",
                "                    model_results['Perplexity'].append(results[1])\n",
                "                    \n",
                "                    pbar.update(1)\n",
                "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
                "    pbar.close()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[(0, 1), (1, 1), (2, 1), (3, 6), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 5), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 5), (24, 1), (25, 2), (26, 3), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 2), (56, 3), (57, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 5), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 2), (97, 1), (98, 1), (99, 2), (100, 1), (101, 1), (102, 1), (103, 3), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 2), (124, 1), (125, 3), (126, 1), (127, 2), (128, 1), (129, 1), (130, 2), (131, 1), (132, 3), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 3), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 4), (148, 3), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 2), (157, 1), (158, 1), (159, 2), (160, 1), (161, 1), (162, 1), (163, 2), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 2), (170, 2), (171, 2), (172, 1), (173, 1), (174, 2), (175, 1), (176, 1), (177, 1), (178, 3), (179, 1), (180, 2), (181, 1), (182, 4), (183, 2)]]\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "  0%|          | 0/540 [04:16<?, ?it/s]\n",
                        "1221it [15:53:18, 79.01s/it] "
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 125,
            "source": [],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "  0%|          | 0/540 [01:23<?, ?it/s]\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "ValueError",
                    "evalue": "Stop argument for islice() must be None or an integer: 0 <= x <= sys.maxsize.",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mlencorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-125-9d172cdb0ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;31m# get the coherence score for the given parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     results = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n\u001b[0;32m---> 64\u001b[0;31m                                                   k=k, a=a, b=b)\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0;31m# Save the model results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Validation_Set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_title\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m<ipython-input-125-9d172cdb0ef3>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(corpus, dictionary, k, a, b)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                            \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                            eta=b)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcoherence_model_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m    525\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input corpus stream has no len(); counting documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mlencorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlencorpus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LdaMulticore.update() called with an empty corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mValueError\u001b[0m: Stop argument for islice() must be None or an integer: 0 <= x <= sys.maxsize."
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.4 64-bit ('3.6.4': pyenv)"
        },
        "interpreter": {
            "hash": "df458b5b017d13255ec6db2b9c4c6a8e38cf4f5c37bd6f0a06e99f86984b0a36"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}